---
title: "Densidade_Condicional_R"
author: "Gabriela Pereira Soares"
date: "13/02/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libs, include=FALSE, message=FALSE}
# Instalação do pacote (rodar só 1x)
#install.packages("devtools")
#library(devtools)
#devtools::install_github("rizbicki/FlexCoDE")

library(FlexCoDE)
library(ggplot2)
```

Vamos carregar os dados já tratados.

```{r data}
trainf0=read.csv("~/TCC/trainf0.csv", encoding = "utf8")
validf0=read.csv("~/TCC/validf0.csv", encoding = "utf8")
teste=read.csv("~/TCC/teste.csv", encoding = "utf8")
```

Vamos também definir novamente as colunas que iremos trabalhar.

```{r cols}
apers = c("auto", "aper_3", "aper_6", "iso", "petro", "PStotal") # abertura isofótica
feat_broad = c('U', 'G', 'R', 'I', 'Z') # banda larga
feat_narrow = c('J0378', 'J0395', 'J0410', 'J0430', 'J0515', 'J0660', 'J0861') # banda estreita

feat = c(feat_broad,feat_narrow)
splus = c()

for (a in apers){
  splus = c(splus,paste0(feat,"_",a))}

# Magnitudes
wise = c("W1_MAG", "W2_MAG")
galex = c('FUVmag', 'NUVmag')

# Erros
error_splus = paste0("e_",splus)
error_wise = paste0("_ERR",wise)
error_galex = paste0("e_",galex)

# Colunas
cols = c('index','Z',splus,wise,galex,error_splus,error_wise,error_galex)
```

```{r plot, include=FALSE}
# Plot density
plot_dens = function(obj,xtest,ztest){
  pred=predict(obj,xtest,B=obj$n_grid,predictionBandProb=FALSE)
  randomOrder=sample(1:nrow(xtest),9,replace=FALSE)
  data=data.frame(x=pred$z,y=pred$CDE[randomOrder[1],],
                  dataPoint=rep(1,length(pred$z)),
                  vertical=ztest[randomOrder[1],])
  for(i in 2:9){
    dataB=data.frame(x=pred$z,
                     y=pred$CDE[randomOrder[i],],
                     dataPoint=rep(i,length(pred$z)),
                     vertical=ztest[randomOrder[1],])
    data=rbind(data,dataB)
  }
  g=ggplot2::ggplot(data,ggplot2::aes(x=x,y=y))+
    ggplot2::geom_line(size=1,color=2)+
    ggplot2::xlab("Response")+
    ggplot2::ylab("Estimated Density")+
    ggplot2::geom_vline(ggplot2::aes(xintercept=vertical),size=1)+
    ggplot2::theme(axis.title=ggplot2::element_text(size=12,face="bold"))+ 
    ggplot2::facet_wrap(~ dataPoint)
  print(g)
}
```


# Densidade Condicional: filtros _narrow_ conseguem melhorar as estimativas de p(z)?

Foi testado anteriormente um modelo com $nIMax=30$. Porém o algoritmo encontrou como melhor I o próprio valor máximo. Assim, aumentamos para 45.

### Modelo 1: Regressor Random Forest sem filtros _narrow_
```{r}
start.time <- Sys.time()

set.seed(47)
fit1=fitFlexCoDE(xTrain=trainf0[c(paste0(feat_broad,"_iso"),wise,galex)],
                 zTrain=trainf0['Z'],
                 xValidation=validf0[c(paste0(feat_broad,"_iso"),wise,galex)],
                 zValidation=validf0['Z'],
                 xTest=teste[c(paste0(feat_broad,"_iso"),wise,galex)],
                 zTest=teste['Z'],
                 nIMax = 45,
                 system='Fourier',
                 regressionFunction = regressionFunction.Forest,
                 regressionFunction.extra = list(nCores=4, ntree=100),
                 chooseDelta = TRUE,
                 chooseSharpen = TRUE,
                 verbose=TRUE)

end.time <- Sys.time()
time.taken1 <- end.time - start.time
time.taken1 # tempo de execução
```

```{r}
print(fit1)
```
```{r}
fit1$estimatedRisk
```
```{r}
fit1$bestI
```
```{r}
fit1$bestDelta
```
```{r}
fit1$bestAlpha
```
### Modelo 2: Random Forest com filtros _narrow_
```{r}
start.time <- Sys.time()

set.seed(47)
fit2=fitFlexCoDE(xTrain=trainf0[c(paste0(feat,"_iso"),wise,galex)],
                 zTrain=trainf0['Z'],
                 xValidation=validf0[c(paste0(feat,"_iso"),wise,galex)],
                 zValidation=validf0['Z'],
                 xTest=teste[c(paste0(feat,"_iso"),wise,galex)],
                 zTest=teste['Z'],
                 nIMax = 45,
                 system='Fourier',
                 regressionFunction = regressionFunction.Forest,
                 regressionFunction.extra = list(nCores=4, ntree=100),
                 chooseDelta = TRUE,
                 chooseSharpen = TRUE,
                 verbose=TRUE)

end.time <- Sys.time()
time.taken2 <- end.time - start.time
time.taken2 # tempo de execução
```

```{r}
print(fit2)
```
```{r}
fit2$estimatedRisk
```
```{r}
# O melhor valor de I foi o próprio máximo (aumentar!)
fit2$bestI
```
```{r}
fit2$bestDelta
```
```{r}
fit2$bestAlpha
```
Vamos comparar a estimativa da densidade condicional de algumas amostras com e sem os filtros _narrow_.

```{r}
xtest1=teste[c(paste0(feat_broad,"_iso"),wise,galex)]
xtest2=teste[c(paste0(feat,"_iso"),wise,galex)]
ztest=teste['Z']
```
```{r}
  pred1=predict(fit1,xtest1,B=fit1$n_grid,predictionBandProb=FALSE)
  pred2=predict(fit2,xtest2,B=fit2$n_grid,predictionBandProb=FALSE)
  
  randomOrder=sample(1:nrow(xtest1),9,replace=FALSE)
  data1=data.frame(x=pred1$z,y=pred1$CDE[randomOrder[1],],
                  dataPoint=rep(1,length(pred1$z)),
                  vertical=ztest[randomOrder[1],])
  data2=data.frame(x=pred2$z,y=pred2$CDE[randomOrder[1],],
                   dataPoint=rep(1,length(pred2$z)),
                   vertical=ztest[randomOrder[1],])
  
  for(i in 2:9){
    dataB1=data.frame(x=pred1$z,
                     y=pred1$CDE[randomOrder[i],],
                     dataPoint=rep(i,length(pred1$z)),
                     vertical=ztest[randomOrder[i],])
    data1=rbind(data1,dataB1)
    dataB2=data.frame(x=pred2$z,
                      y=pred2$CDE[randomOrder[i],],
                      dataPoint=rep(i,length(pred2$z)),
                      vertical=ztest[randomOrder[i],])
    data2=rbind(data2,dataB2)
  }
```
```{r}
g_rf=ggplot() + geom_line(data=data2, aes(x=x, y = y, color="Com narrow-bands"),size=1) +
geom_line(data=data1, aes(x=x,y=y,color="Sem narrow-bands"),size=1)+
    geom_vline(data=data1,aes(xintercept=vertical),size=1)+
    facet_wrap(~ dataPoint)+
    scale_color_manual(values = c("Com narrow-bands" = "red", "Sem narrow-bands" = "blue"))+
    labs(color = "")
print(g_rf)
```


### Modelo 2.2: Random Forest com filtros _narrow_ e nIMax incrementado

```{r}
start.time <- Sys.time()

set.seed(47)
fit2_2=fitFlexCoDE(xTrain=trainf0[c(paste0(feat,"_iso"),wise,galex)],
                 zTrain=trainf0['Z'],
                 xValidation=validf0[c(paste0(feat,"_iso"),wise,galex)],
                 zValidation=validf0['Z'],
                 xTest=teste[c(paste0(feat,"_iso"),wise,galex)],
                 zTest=teste['Z'],
                 nIMax = 100,
                 system='Fourier',
                 regressionFunction = regressionFunction.Forest,
                 regressionFunction.extra = list(nCores=6, ntree=100), #ntree=250
                 chooseDelta = TRUE,
                 chooseSharpen = TRUE,
                 verbose=TRUE)

end.time <- Sys.time()
time.taken2_2 <- end.time - start.time
time.taken2_2 # tempo de execução
```
```{r}
fit2_2$estimatedRisk
```
```{r}
fit2_2$bestI
```
Acredito que o ganho com a inclusão de tantos coeficientes foi bem pouco quando comparado o risco estimado e o erro padrão com o modelo com 45 coeficientes.

```{r}
plot(fit2_2$errors)
```


```{r}
plot_dens(fit2_2,teste[c(paste0(feat,"_iso"),wise,galex)],teste['Z'])
```
Não parece ter ficado bom...

### Modelo 2.3: Random Forest com filtros _narrow_ e nIMax incrementado

```{r}
start.time <- Sys.time()

set.seed(47)
fit2_3=fitFlexCoDE(xTrain=trainf0[c(paste0(feat,"_iso"),wise,galex)],
                 zTrain=trainf0['Z'],
                 xValidation=validf0[c(paste0(feat,"_iso"),wise,galex)],
                 zValidation=validf0['Z'],
                 xTest=teste[c(paste0(feat,"_iso"),wise,galex)],
                 zTest=teste['Z'],
                 nIMax = 100,
                 system='Fourier',
                 regressionFunction = regressionFunction.Forest,
                 regressionFunction.extra = list(nCores=6, ntree=250), #ntree=250
                 chooseDelta = TRUE,
                 chooseSharpen = TRUE,
                 verbose=TRUE)

end.time <- Sys.time()
time.taken2_3 <- end.time - start.time
time.taken2_3 # tempo de execução
```
```{r}
plot(fit2_3$errors)
```
```{r}
fit2_3$bestI
```
```{r}
fit2_3$estimatedRisk
```
```{r}
plot_dens(fit2_3,teste[c(paste0(feat,"_iso"),wise,galex)],teste['Z'])
```
Vamos comparar o modelo com 95 parametros com o modelo de 45 parametros, ambos com narrow bands.

```{r}
xtest1=teste[c(paste0(feat_broad,"_iso"),wise,galex)]
ztest=teste['Z']
```
```{r}
  pred1=predict(fit2_3,xtest1,B=fit1$n_grid,predictionBandProb=FALSE)
  pred2=predict(fit2,xtest1,B=fit2$n_grid,predictionBandProb=FALSE)
  
  randomOrder=sample(1:nrow(xtest1),9,replace=FALSE)
  data1=data.frame(x=pred1$z,y=pred1$CDE[randomOrder[1],],
                  dataPoint=rep(1,length(pred1$z)),
                  vertical=ztest[randomOrder[1],])
  data2=data.frame(x=pred2$z,y=pred2$CDE[randomOrder[1],],
                   dataPoint=rep(1,length(pred2$z)),
                   vertical=ztest[randomOrder[1],])
  
  for(i in 2:9){
    dataB1=data.frame(x=pred1$z,
                     y=pred1$CDE[randomOrder[i],],
                     dataPoint=rep(i,length(pred1$z)),
                     vertical=ztest[randomOrder[i],])
    data1=rbind(data1,dataB1)
    dataB2=data.frame(x=pred2$z,
                      y=pred2$CDE[randomOrder[i],],
                      dataPoint=rep(i,length(pred2$z)),
                      vertical=ztest[randomOrder[i],])
    data2=rbind(data2,dataB2)
  }
```
```{r}
g_rf=ggplot() + geom_line(data=data2, aes(x=x, y = y, color="I=45"),size=1) +
geom_line(data=data1, aes(x=x,y=y,color="I=95"),size=1)+
    geom_vline(data=data1,aes(xintercept=vertical),size=1)+
    facet_wrap(~ dataPoint)+
    scale_color_manual(values = c("I=45" = "red", "I=95" = "blue"))+
    labs(color = "")
print(g_rf)
```

### Modelo 3: XGBoosting sem filtros _narrow_

```{r}
start.time <- Sys.time()

set.seed(47)
fit3=fitFlexCoDE(xTrain=trainf0[c(paste0(feat_broad,"_iso"),wise,galex)],
                 zTrain=trainf0['Z'],
                 xValidation=validf0[c(paste0(feat_broad,"_iso"),wise,galex)],
                 zValidation=validf0['Z'],
                 xTest=teste[c(paste0(feat_broad,"_iso"),wise,galex)],
                 zTest=teste['Z'],
                 nIMax = 45,
                 system='Fourier',
                 regressionFunction = regressionFunction.XGBoost,
                 regressionFunction.extra = list(nCores=4, ninter=500),
                 chooseDelta = TRUE,
                 chooseSharpen = TRUE,
                 verbose=TRUE)

end.time <- Sys.time()
time.taken3 <- end.time - start.time
time.taken3 # tempo de execução
```

```{r}
print(fit3)
```
```{r}
fit3$estimatedRisk
```
```{r}
fit3$bestI
```
```{r}
fit3$bestDelta
```
```{r}
fit3$bestAlpha
```

### Modelo 4: XGBoosting com filtros _narrow_
```{r}
start.time <- Sys.time()

set.seed(47)
fit4=fitFlexCoDE(xTrain=trainf0[c(paste0(feat,"_iso"),wise,galex)],
                 zTrain=trainf0['Z'],
                 xValidation=validf0[c(paste0(feat,"_iso"),wise,galex)],
                 zValidation=validf0['Z'],
                 xTest=teste[c(paste0(feat,"_iso"),wise,galex)],
                 zTest=teste['Z'],
                 nIMax = 45,
                 system='Fourier',
                 regressionFunction = regressionFunction.XGBoost,
                 regressionFunction.extra = list(nCores=4, ninter=500),
                 chooseDelta = TRUE,
                 chooseSharpen = TRUE,
                 verbose=TRUE)

end.time <- Sys.time()
time.taken4 <- end.time - start.time
time.taken4 # tempo de execução
```

```{r}
print(fit4)
```

Nota-se que, no caso do regressor XGBoosting, houve menor importância das variáveis _narrow bands_. Além disso, as variáveis W2_MAG e FUVmag tiveram MUITA importância no modelo, ao contrário do Random Forest que tiveram baixíssima importância. O que é estranho, dado que FUVmag tem mais de 70% de dados faltantes.

O que me parece é que, de maneira geral, houve uma inversão. Ou seja, as variáveis mais importantes no Random Forest tiver menor importância no XGBoosting.

*Questão:* A importância média mostra quais variáveis foram mais importantes (apareceram mais vezes no regressor) para estimar os coeficientes de expansão? Consequentemente, como os coeficientes estão envolvidos no cálculo da densidade, ele traz informações de quais variáveis são mais importantes para estimação da densidade condicional?

```{r}
fit4$estimatedRisk
```
O modelo com XGBoosting teve maior valor de risco do que o modelo com Random Forest. O erro padrão não teve muita diferença.

```{r}
fit4$bestI
```
```{r}
fit4$bestDelta
```
```{r}
fit4$bestAlpha
```

Vamos comparar a estimativa da densidade condicional de algumas amostras com e sem os filtros _narrow_.

```{r}
pred3=predict(fit3,xtest1,B=fit3$n_grid,predictionBandProb=FALSE)
pred4=predict(fit4,xtest2,B=fit4$n_grid,predictionBandProb=FALSE)

randomOrder=sample(1:nrow(xtest1),9,replace=FALSE)
data3=data.frame(x=pred3$z,y=pred3$CDE[randomOrder[1],],
                 dataPoint=rep(1,length(pred3$z)),
                 vertical=ztest[randomOrder[1],])
data4=data.frame(x=pred4$z,y=pred4$CDE[randomOrder[1],],
                 dataPoint=rep(1,length(pred4$z)),
                 vertical=ztest[randomOrder[1],])

for(i in 2:9){
  dataB3=data.frame(x=pred3$z,
                    y=pred3$CDE[randomOrder[i],],
                    dataPoint=rep(i,length(pred3$z)),
                    vertical=ztest[randomOrder[i],])
  data3=rbind(data3,dataB3)
  dataB4=data.frame(x=pred4$z,
                    y=pred4$CDE[randomOrder[i],],
                    dataPoint=rep(i,length(pred4$z)),
                    vertical=ztest[randomOrder[i],])
  data4=rbind(data4,dataB4)
}
```
```{r}
g_rf=ggplot() + geom_line(data=data4, aes(x=x, y = y, color="Com narrow-bands"),size=1) +
geom_line(data=data3, aes(x=x,y=y,color="Sem narrow-bands"),size=1)+
    geom_vline(data=data3,aes(xintercept=vertical),size=1)+
    facet_wrap(~ dataPoint)+
    scale_color_manual(values = c("Com narrow-bands" = "red", "Sem narrow-bands" = "blue"))+
    labs(color = "")
print(g_rf)
```
OBS.: Não consegui guardar o tempo, mas o modelo com Random Forest demorou cerca de 1h30min para rodar. Já o modelo com XGBoosting demorou mais de 4hs para rodar, então provavelmente é um modelo que não vale a pena devido também ao seu alto custo computacional.

### Questões: 
- O fato do modelo 2 possuir muitos parâmetros não torna o modelo complexo demais? Não corre risco de overfitting?
- Alterar outros parâmetros? Ex.: base cosine, ntree, niter...?
- Testar modelos com mais variáveis? Ex.: cores, erros...?